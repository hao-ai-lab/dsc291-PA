{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Systems Homework 2\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/hao-ai-lab/dsc291-PA/blob/main/pa2/mlsys_hw2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "**Homework due: June 2, 2024, 11:59 pm, PDT**.\n",
    "\n",
    "Optimizing tensor programs is crucial in machine learning compilation as it ensures that operators like matrix multiplication, convolution, and reduction run efficiently on different hardware. \n",
    "In this assignment, you'll utilize a tensor program optimization interface within [TVM](https://tvm.apache.org) to schedule these common operators effectively.\n",
    "\n",
    "* **Be prepared** -- in this homework you may need to learn many new terminologies and concepts.\n",
    "* You should work on this homework **individually** -- it is not a team homework.\n",
    "* This homework **requires** NVIDIA GPU with CUDA environment. You can do the homework on Google Colab (by clicking the badge above) or any other GPU server that you have access to. **Important note: we will test your submission on Google Colab. So if you do the homework on other environments, we recommend you to test it on Colab before submission.**\n",
    "* This homework is pure Python. No C++ is needed in this homework.\n",
    "* Please check out the end of this notebook for the homework submission requirement.\n",
    "* Please do not share your solution on publicly available websites (e.g., GitHub).\n",
    "* **About testing and grading.** You can submit your homework for multiple times. Your submission (only your last one) will be read and graded **manually** after the homework deadline. **This means you will not be able to see your scores immediately after submission.** The content of this notebook below describes some possible ways for you to check and test by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Using Google Colab\n",
    "\n",
    "If you are using Google Colab environment, please make a copy of this notebook file by selecting \"Save a copy in Drive\" from the \"File\" menu, and then run the code block below to set up workspace. After cloning, you will see the cloned repository in the \"Files\" bar on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p dsc291\n",
    "%cd /content/drive/MyDrive/dsc291\n",
    "!git clone https://github.com/hao-ai-lab/dsc291-PA.git\n",
    "%cd /content/drive/MyDrive/dsc291/dsc291-PA/pa2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following cell to install TVM dependency.\n",
    "\n",
    "**Note.** The PyPI package installed below only has CPU support.\n",
    "It is fine to use this CPU package when you implement the five tasks of part 1.\n",
    "We will have instructions on how to install the GPU package\n",
    "later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install mlc-ai -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate that the dependency is successfully installed by running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.10/dist-packages/tvm']\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "tvm.__path__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using local environment\n",
    "\n",
    "If you are using local/server environment (requiring NVIDIA GPU and CUDA â‰¥ 11.7), please clone this repository.\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/hao-ai-lab/dsc291-PA.git\n",
    "cd dsc291-PA/pa2\n",
    "export PYTHONPATH=.:$PYTHONPATH\n",
    "```\n",
    "\n",
    "We recommend you to create a conda environment and install the dependency package in the environment.\n",
    "```shell\n",
    "conda create --name dsc291 python=3.11\n",
    "conda activate dsc291\n",
    "# Replace \"cu118\" with your local CUDA version (e.g., cu121, cu122, etc.)\n",
    "python3 -m pip install --force-reinstall mlc-ai-cu118 -f https://mlc.ai/wheels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please also select the Python interpreter in this conda environment and use it to run this notebook.\n",
    "We can validate that the dependency is successfully installed by running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/path/to/conda/envs/15442/lib/python3.11/site-packages/tvm']\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "tvm.__path__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Optimize Matrix Multiplication (80 pt)\n",
    "\n",
    "In this part, we will walk you through the process of manually optimize and fuse the \"\"GeMM + ReLU + add\" operator with TVM\n",
    "and run it on the NVIDIA GPU in your environment.\n",
    "\n",
    "The \"GeMM + ReLU + add\" pattern is common in machine learning models when activation\n",
    "and residual-add operations are involved.\n",
    "Formally, for given 2-dim tensors $A$, $B$, and $C$, we want to compute the following $D$:\n",
    "\n",
    "$$\n",
    "D = \\mathrm{ReLU} (A B) + C.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define this operator using [TensorIR](https://arxiv.org/pdf/2207.04296.pdf),\n",
    "the tensor program level intermediate representation (IR) in TVM, as follows.\n",
    "You can also find this definition in `gemm_relu_add.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tvm import tir\n",
    "from tvm.script import tir as T\n",
    "\n",
    "M = 2048\n",
    "N = 2048\n",
    "K = 2048\n",
    "\n",
    "\n",
    "@T.prim_func\n",
    "def gemm_relu_add(\n",
    "    A: T.Buffer((M, K), \"float32\"),\n",
    "    B: T.Buffer((K, N), \"float32\"),\n",
    "    C: T.Buffer((M, N), \"float32\"),\n",
    "    D: T.Buffer((M, N), \"float32\"),\n",
    ") -> None:\n",
    "    matmul = T.alloc_buffer((M, N), \"float32\", scope=\"global\")\n",
    "    relu = T.alloc_buffer((M, N), \"float32\", scope=\"global\")\n",
    "    # Compute GeMM\n",
    "    for i, j, k in T.grid(M, N, K):\n",
    "        with T.block(\"gemm\"):\n",
    "            vi = T.axis.spatial(M, i)\n",
    "            vj = T.axis.spatial(N, j)\n",
    "            vk = T.axis.reduce(K, k)\n",
    "            with T.init():\n",
    "                matmul[vi, vj] = T.float32(0)\n",
    "            matmul[vi, vj] += A[vi, vk] * B[vk, vj]\n",
    "    # Compute ReLU\n",
    "    for i, j in T.grid(M, N):\n",
    "        with T.block(\"relu\"):\n",
    "            vi = T.axis.spatial(M, i)\n",
    "            vj = T.axis.spatial(N, j)\n",
    "            relu[vi, vj] = T.max(matmul[vi, vj], T.float32(0))\n",
    "    # Compute add\n",
    "    for i, j in T.grid(M, N):\n",
    "        with T.block(\"add\"):\n",
    "            vi = T.axis.spatial(M, i)\n",
    "            vj = T.axis.spatial(N, j)\n",
    "            D[vi, vj] = relu[vi, vj] + C[vi, vj]\n",
    "\n",
    "\n",
    "sch = tir.Schedule(gemm_relu_add)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the above code and look into its components.\n",
    "\n",
    "* The function is designed to take four tensors `A`, `B`, `C` and `D` as inputs, each with thier corresponding share and dtype.\n",
    "Notably, D is passed in as an argument rather than being returned by the function. This approach, known as \"destination-passing style,\" \n",
    "involves pre-allocating the destination tensor before calling the function and then passing it in as an argument.\n",
    "This style is commonly used in low-level operator libraries like [cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html) and in tensor program abstractions within machine learning compilers.\n",
    "* The initial two lines within the function body establish tensors for storing intermediate results from the GeMM and ReLU operations. \n",
    "At this stage, their memory scope is global.\n",
    "* Next, there are the three sequential for-loop blocks defining the computation.\n",
    "  * The first one defines the GeMM computation. For a given `vi` and `vj`, the loop body\n",
    "  reads from `A` and `B`, sums up the values along `vk`, and stores the summation result into\n",
    "  the intermediate tensor `matmul`, with `T.float32(0)` being the initial value of the summation.\n",
    "  * The second one defines the ReLU computation, which element-wisely takes the maximum\n",
    "  of `matmul` and zero.\n",
    "  * The third one describes the element-wise addition of the ReLU result and the input tensor `C`.\n",
    "* After defining the function, we create a `tir.Schedule` instance with regard to this function.\n",
    "`tir.Schedule` is the core tool in this homework you will use to optimize this function.\n",
    "`tir.Schedule` provides the a set of function transformations (which we call \"schedule primitives\")\n",
    "that could accelerate the function execution on hardware.\n",
    "We will introduce these schedule primitives later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this part is to optimize this GeMM + ReLU + add workload so that it can efficiently run on GPU.\n",
    "Usually, optimizing such a workload of matrix multiplication with epilogues (e.g., ReLU and add here)\n",
    "for GPU, five major steps are required:\n",
    "\n",
    "1. shared memory tiling,\n",
    "2. register tiling,\n",
    "3. cooperative fetching,\n",
    "4. write cache.\n",
    "5. epilogue fusion.\n",
    "\n",
    "In grading your submission, we will access your optimization from two perpectives:\n",
    "* Firstly, we will ensure numerical correctness remains intact post-optimization. This means that \n",
    "your optimized codes should yield the same numerical results as the original naive implementation.\n",
    "We have provided the tests for you to ocheck the numerical correctness of your optimization.\n",
    "But you may not be able to run the tests before you have implemented all optimizations.\n",
    "* Secondly, we will evaluate whether you have correctly implemented the optimizations outlined above.\n",
    "While there is no automated test for this, we will manually review your submission to ensure each task\n",
    "is properly addressed. Additionally, we have provided a reference implementation (`reference.py`) as your\n",
    "guidance to complete all five tasks, **though your final function need not match it exactly**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Shared memory tiling (20 pt)\n",
    "\n",
    "Our optimization primarily focuses on enhancing the GeMM (General Matrix Multiply) component. \n",
    "GeMM involves the most intensive computation and memory accesses among the three stages (GeMM, ReLU, and addition) in this workload.\n",
    "By optimizing GeMM, subsequent optimizations for the remaining parts can be tailored based on the improvements made to GeMM.\n",
    "\n",
    "You have learned about the concept of **thread blocks** in GPU architectures from the lectures.\n",
    "To optimize a GeMM workload, people usually partition the destination matrix (`matmul` for our case)\n",
    "into multiple tiles along both the row and column dimensions, and assign one thread block\n",
    "for the computation of each tile.\n",
    "\n",
    "For example, in the figure below, a thread block computes a `(tile_x, tile_y)` tile.\n",
    "That is to say, this particular thread block will iterate over a `(tile_x, K)` region of `A`\n",
    "and a `(K, tile_y)` region of `B`, compute the matrix multiplication of these two regions,\n",
    "and write the results into the `(tile_x, tile_y)` tile.\n",
    "We can use Python notation to formally describe the tiling:\n",
    "for a given thread block, assuming the tile that this thread block computes\n",
    "starts from `offset_x` on the `M` dimension and `offset_y` on the `N` dimension,\n",
    "then the thread block computes the following:\n",
    "```python\n",
    "matmul[offset_x : offset_x + tile_x, offset_y : offset_y + tile_y] = (\n",
    "    A[offset_x : offset_x + tile_x, :] @ B[:, offset_y : offset_y + tile_y]\n",
    ")\n",
    "```\n",
    "\n",
    "<img src=\"figure/shared-memory-tiling.jpg\" alt=\"figure/shared-memory-tiling.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is about reducing the memory access pressure of the GeMM operator.\n",
    "This is done via leveraging **shared memory** available in GPU, which you also learned about from lectures:\n",
    "rather than directly reading from `A` and `B` (which reside in global memory)\n",
    "at the time of computing multiplication, each thread block first loads the `A` region\n",
    "and `B` region it needs from global memory to shared memory.\n",
    "\n",
    "Just like CPU cache, while shared memory can offer much faster access speed,\n",
    "it has relatively smaller size than global memory.\n",
    "Usually, the size of shared memory available in a thread block is no more than 48KB.\n",
    "In our case, given each float32 element has four bytes,\n",
    "the `A` region and `B` region that a thread block needs\n",
    "have size `(tile_x * K * 4) + (tile_y * K * 4)` byte,\n",
    "which is far more than the available shared memory size a thread block can use.\n",
    "To address this issue, you also need split the long stripe of `A` and `B` into\n",
    "multiple chunks along the `K` dimension.\n",
    "By doing this, now you are able to go through `A` chunks and `B` chunks one at a time simultaneously,\n",
    "and load a single `A` tile and `B` chunk into shared memory, which fits the available\n",
    "shared memory size.\n",
    "This can be formally written as\n",
    "\n",
    "```python\n",
    "matmul[offset_x : offset_x + tile_x, offset_y : offset_y + tile_y] = sum(\n",
    "    A[offset_x : offset_x + tile_x, k : k + tile_k]\n",
    "    @ B[k : k + tile_k, offset_y : offset_y + tile_y]\n",
    "    for k in range(0, n, tile_k)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze how much global memory accesses we reduce by leveraging shared memory:\n",
    "\n",
    "* There are $M \\times K$ elements in $A$ and $N \\times K$ elements in $B$.\n",
    "When not using shared memory, each element of $A$ is read for $N$ times,\n",
    "and each element of $B$ is read for $M$ times.\n",
    "So number of accesses to $A$ and $B$ in global memory are both $MNK$.\n",
    "* With shared memory tiling, the number of times each element of $A$ is read becomes $\\frac{N}{\\mathrm{tile}_y}$,\n",
    "and the number of times each element of $B$ is read becomes $\\frac{M}{\\mathrm{tile}_x}$.\n",
    "And the total number of accesses to $A$ and $B$ are divided by $\\mathrm{tile}_y$ and $\\mathrm{tile}_x$ respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the shared memory tiling in the `shared_memory_tiling` function in `gemm_relu_add.py`.\n",
    "Below are the schedule primitives you will use in this task.\n",
    "We provide [another Jupyter notebook](schedule_example.ipynb) that walks you through an example\n",
    "of using some of these schedule primitives for your reference.\n",
    "Meanwhile, you can click the links below to check out their documentations,\n",
    "which contain a basic example on how to use the primitive.\n",
    "We also provide instructions in your TODO area in `gemm_relu_add.py`.\n",
    "\n",
    "- `split` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.split)\n",
    "  - It splits one loop into multiple loops that collectively iterate the iteration space of the original loop.\n",
    "  We use `split` for tiling.\n",
    "- `reorder` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.reorder)\n",
    "  - It reorders the specified contiguous loops into a new order.\n",
    "  We apply `reorder` for loops after splitting for tiling.\n",
    "- `bind` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.bind)\n",
    "  - It binds a loop to `blockIdx.x/y/z` or `threadIdx.x/y/z` on GPU,\n",
    "  so that we can specify the area to compute of a thread block or a single thread.\n",
    "- `cache_read` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.cache_read)\n",
    "  - It generates a cache stage for the specified region in the specified memory scope.\n",
    "  We use `cache_read` to generate the read stages from global memory to shared memory.\n",
    "- `compute_at` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.compute_at)\n",
    "  - It moves a computation (e.g., the shared memory read stage) to the location under the specified loop.\n",
    "  We use `compute_at` to move the shared memory read stages to the right location.\n",
    "\n",
    "You can run `gemm_relu_add.py` to see how the function looks like after your optimization.\n",
    "Note: you can insert `sch.show()` at any position to print out the function,\n",
    "so that you can visually see how the function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gemm_relu_add.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Register tiling (20 pt)\n",
    "\n",
    "In the previous subsection, we focus on accelerating the `matmul` region that **a thread block collectively computes**.\n",
    "In this subsection, we dive deeper to figure out the `matmul` region that **a single thread computes**.\n",
    "\n",
    "The idea is pretty much similar to the shared memory tiling.\n",
    "We can also let a single thread compute a tile in `matmul`.\n",
    "For example, we assign each thread in a thread block to compute a tile of size `(thread_tile_x, thread_tile_y)`.\n",
    "And moreover, rather than directly load `A` and `B` from shared memory when computing the tile,\n",
    "we again first load `A` and `B` from shared memory to **local registers**.\n",
    "Registers are local memory in a running CUDA thread that is running,\n",
    "and can be accessed in much shorter time than shared memory.\n",
    "Similarly, we also need to do thread-level tiling along the `K` dimension,\n",
    "because each thread has very limited number of available registers.\n",
    "And our goal is to make sure the data that we want to load from `A` and `B` in each thread-level tile can fit into the registers.\n",
    "With register tiling, we use $(\\mathrm{thread\\_tile}_x + \\mathrm{thread\\_tile}_y) \\times \\mathrm{thread\\_tile}_k$\n",
    "registers for reading data from the shared memory of `A` and `B` in total.\n",
    "\n",
    "\n",
    "<img src=\"figure/register-tiling.jpg\" alt=\"figure/register-tiling.jpg\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It worths noting that, by fixing the thread tile size, we equivalently fixed the number\n",
    "of threads in each thread block:\n",
    "a thread block computes a tile of `(tile_x, tile_y)`, and a single thread computes a tile of\n",
    "`(thread_tile_x, thread_tile_y)`.\n",
    "So in a thread block, `blockDim.x` is `tile_x / thread_tile_x`, `blockDim.y` is `tile_y / thread_tile_y`,\n",
    "and the total number of threads in a thread block is `(tile_x / thread_tile_x) * (tile_y / thread_tile_y)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the register tiling in the `register_tiling` function in `gemm_relu_add.py`.\n",
    "As mentioned above, your implementation of register tiling should follow the same pattern and steps\n",
    "as your implementation of shared memory tiling.\n",
    "So make sure you understand each step of the shared memory tiling before starting this task.\n",
    "The schedule primitives you will use in this task is the same as task 1.\n",
    "And please note that for register tiling, you should use `\"local\"` (which means local registers)\n",
    "as the scope of `cache_read`, rather than `\"shared\"` which you used in task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gemm_relu_add.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Cooperative fetching (20 pt)\n",
    "\n",
    "In both shared memory tiling and register tiling, we've designated a `matmul` region \n",
    "for each thread to compute, completing our setup for the GeMM computation. However, \n",
    "there are still a few optimization steps remaining. The first one is **cooperative fetching**, \n",
    "focusing on optimizing the memory load from global memory to shared memory for both matrices `A` and `B`.\n",
    "\n",
    "In the \"shared memory tiling\" subsection, we established stages for a thread block \n",
    "to fetch data from matrices `A` and `B` into their respective shared memory. \n",
    "However, the specific manner in which the data is fetched remains unspecified. \n",
    "Particularly, we haven't specified which elements of `A`/`B` each individual thread \n",
    "should read. Without this specification, each thread within a thread block will, \n",
    "by default, load the entire `A`/`B` regions assigned to the thread block into the shared memory. \n",
    "As shared memory is accessible to all threads within the block, this default behavior \n",
    "results in redundant global memory reads and shared memory writes, \n",
    "failing to effectively reduce the number of global memory accesses.\n",
    "\n",
    "This motivates us to implement cooperative fetching.\n",
    "Cooperative fetching means all the threads in a thread block\n",
    "collectively (\"*cooperative*\") load values from global memory to shared memory (\"*fetching*\").\n",
    "To make sure each element in the tile is read only once (by some thread),\n",
    "we can divide the number of elements in the shared memory tile evenly by\n",
    "the number of threads, yielding the number of elements each thread loads.\n",
    "Then, we let all the threads in the thread block to load values in memory order.\n",
    "\n",
    "For our example, we can calculate that each thread in a thread block needs to\n",
    "load `thread_tile_x * thread_tile_y * tile_k / tile_y` elements from `A`,\n",
    "and likewise, `thread_tile_x * thread_tile_y * tile_k / tile_x` elements from `B`.\n",
    "If we denote the number of total threads (`(tile_x / thread_tile_x) * (tile_y / thread_tile_y)`) as\n",
    "`num_threads`, then when loading an `A` tile to shared memory, in the first round all the threads\n",
    "load the first continuous `num_threads` elements, in the second round they load\n",
    "the next continuous `num_threads` elements, and keep loading for\n",
    "`thread_tile_x * thread_tile_y * tile_k / tile_y` times.\n",
    "This applies to the shared memory load of `B` in the same way.\n",
    "\n",
    "[The other Jupyter notebook](schedule_example.ipynb) contains a simple example of\n",
    "cooperative fetching that you might find helpful for understanding,\n",
    "if you have not went through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the cooperative fetching in the `cooperative_fetching` function in `gemm_relu_add.py`.\n",
    "As you can see in the code, since the cooperative fetching for `A` and `B` are the same,\n",
    "we can write a common schedule function and apply it to the shared memory read stages\n",
    "of both `A` and `B`.\n",
    "The new schedule primitive you will use in this task is\n",
    "`fuse` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.fuse),\n",
    "which, on the contrary to `split`, fuses multiple loops into a single loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gemm_relu_add.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Write Cache (10 pt)\n",
    "\n",
    "Now, we proceed to the stage of writing the GeMM computation results into `matmul`. \n",
    "Up to this point, our GeMM computation still directly writes the results into the global \n",
    "memory of `matmul`, with each location of `matmul` being written to `K` times due to \n",
    "the summation over the `K` dimension. Clearly, this approach is suboptimal.\n",
    "\n",
    "Instead of directly writing the computation results into global memory, \n",
    "we can accumulate the summation in local registers. Subsequently, we can write \n",
    "the final result into global memory fewer times after the accumulation process is completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the write cache in the `write_cache` function in `gemm_relu_add.py`.\n",
    "In this task, you will use the schedule primitive\n",
    "`cache_write` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.cache_write)\n",
    "and\n",
    "`reverse_compute_at` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.reverse_compute_at)\n",
    "to put the write cache stage at the right location.\n",
    "Similar to `cache_read` which generates a read cache stage for a computation,\n",
    "`cache_write` generates a write cache stage for a computation at the specified scope.\n",
    "`reverse_compute_at` works in basically the same way as `compute_at`,\n",
    "with the difference being `compute_at` moves a computation **in front of** the target loop into the target loop,\n",
    "while `reverse_compute_at` moves a computation **behind** the target loop into the target loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gemm_relu_add.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Epilogue Fusion (10 pt)\n",
    "\n",
    "The preceding subsections have covered common techniques utilized to optimize a \n",
    "GeMM workload. Now, we shift our focus to the ReLU and addition operators. \n",
    "While these operators are computationally lighter compared to GeMM, they still \n",
    "consume a considerable amount of time due to their direct read/write operations \n",
    "from/to global memory. This inefficiency can be addressed by incorporating the \n",
    "lightweight epilogue operators (ReLU and addition) directly into the write-back \n",
    "stage of the scheduled GeMM workload. During this stage, the results of GeMM are \n",
    "still stored in registers, optimizing the elimination of redundant memory accesses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the epilogue fusion in the `epilogue_fusion` function in `gemm_relu_add.py`,\n",
    "where you will use `get_block` to retrieve the computation of addition and ReLU,\n",
    "and use `reverse_compute_inline` [(docs)](https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule.reverse_compute_inline)\n",
    "to inline the addition and ReLU into the write-back stage of GeMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gemm_relu_add.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Evaluating your optimization (10 pt)\n",
    "\n",
    "Now that you have completed all five steps of the optimization. Please compare \n",
    "the execution time of your optimized implementation and that of the naive implementation by following the instructions below and report the time in the \n",
    "file [time_difference.txt](time_difference.txt).\n",
    "\n",
    "Please also try to tune the tile size in your optimized implementation, report how the time changed and came up with some possible reasons in the file [time_difference.txt](time_difference.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are using Google Colab...\n",
    "\n",
    "Now we need to switch to GPU runtime and install the PyPI package with CUDA support before\n",
    "running tests.\n",
    "Please go to \"Runtime - Change runtime type\" in the navigation bar, and select \"T4 GPU\".\n",
    "Then run the cells below to install the PyPI package, verify the installation,\n",
    "and mount the Google Drive again.\n",
    "\n",
    "**Note 1.** Be sure to change runtime type to use T4 GPU.\n",
    "Or otherwise you will see library error when importing TVM in Python below.\n",
    "\n",
    "**Note 2.** Please disconnect the GPU runtime when you are not using it.\n",
    "Otherwise you may hit the Colab GPU usage limit and will be temporarily not able to\n",
    "connect to a GPU in Colab if you are a free Colab user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install mlc-ai-cu122 -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "tvm.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/dsc291/dsc291-PA/pa2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have finished implementing all the optimizations,\n",
    "you can test the numerical correctness of your code via running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing test round 0...\n",
      "Passing test round 1...\n",
      "Passing test round 2...\n",
      "Passing test round 3...\n",
      "Passing test round 4...\n",
      "Passed all tests.\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluate.py --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate the execution time of function after your optimization via running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluate.py --evaluate-manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you get a sense of how much performance your optimization gains,\n",
    "we can evaluate a naive GeMM + ReLU + add function with little optimizations.\n",
    "Ideally, you should find that your optimization is times faster than the naive optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluate.py --evaluate-naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out the CUDA source code of your scheduled workload that TVM generates,\n",
    "and compare it with the Python script you printed\n",
    "via `sch.show()`.\n",
    "As a practice, try to point out which parts in the CUDA source code\n",
    "are the shared memory, local registers,\n",
    "the main GeMM computation, cooperative fetching, write cache,\n",
    "and the fused epilogue respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 evaluate.py --show-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Homework Feedback (0 pt)\n",
    "\n",
    "This is the first time we offer this course, and we appreciate any homework feedback from you.\n",
    "You can leave your feedback (if any) in `feedback.txt`, and submit it together with the source code.\n",
    "Possible choices can be:\n",
    "\n",
    "- Are the notebooks and instructions clear enough?\n",
    "- How difficult do you think this homework is?\n",
    "- How much time does the homework take? Which task takes the most time?\n",
    "- Which part of the homework do you feel hard to understand?\n",
    "- And any other things you would like to share.\n",
    "\n",
    "Your feedback will be very useful in helping us improve the homework quality\n",
    "for next years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Submit Your Homework\n",
    "\n",
    "In the home directory for the assignment, execute the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make handin.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create an archive file with `gemm_relu_add.py`, `time_difference.txt` and `feedback.txt`.\n",
    "You can check the contents of `handin.tar` to make sure it contains all the needed files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -tf handin.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected to list the three files:\n",
    "```\n",
    "gemm_relu_add.py\n",
    "time_difference.txt\n",
    "feedback.txt\n",
    "```\n",
    "\n",
    "Then, please go to Gradescope and submit the file `handin.tar`.\n",
    "\n",
    "This assignment is not automatically graded, and you will not receive immediate feedback upon submission.\n",
    "You can submit multiple times, but only your final submission will be graded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
